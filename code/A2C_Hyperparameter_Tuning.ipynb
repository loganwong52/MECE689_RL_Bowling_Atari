{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyNnTM5xr0riT3vtx66rsCzU"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"8a15214340b84d34bd86f444bcc43083":{"model_module":"@jupyter-widgets/output","model_name":"OutputModel","model_module_version":"1.0.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/output","_model_module_version":"1.0.0","_model_name":"OutputModel","_view_count":null,"_view_module":"@jupyter-widgets/output","_view_module_version":"1.0.0","_view_name":"OutputView","layout":"IPY_MODEL_1e5bba4628d24218be669ed03436da39","msg_id":"","outputs":[{"output_type":"display_data","data":{"text/plain":"\u001b[35m 100%\u001b[0m \u001b[38;2;249;38;114m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[38;2;249;38;114m╸\u001b[0m \u001b[32m49,990/50,000 \u001b[0m [ \u001b[33m0:05:30\u001b[0m < \u001b[36m0:00:01\u001b[0m , \u001b[31m156 it/s\u001b[0m ]\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080\"> 100%</span> <span style=\"color: #f92672; text-decoration-color: #f92672\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸</span> <span style=\"color: #008000; text-decoration-color: #008000\">49,990/50,000 </span> [ <span style=\"color: #808000; text-decoration-color: #808000\">0:05:30</span> &lt; <span style=\"color: #008080; text-decoration-color: #008080\">0:00:01</span> , <span style=\"color: #800000; text-decoration-color: #800000\">156 it/s</span> ]\n</pre>\n"},"metadata":{}}]}},"1e5bba4628d24218be669ed03436da39":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}}}}},"cells":[{"cell_type":"markdown","source":["ACTOR 2 CRITIC with HYPERPARAMETER TUNING\n","\n","Logan Wong\n","\n","law3082"],"metadata":{"id":"INW0WxGUHM18"}},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kKgz8oXKHBy7","executionInfo":{"status":"ok","timestamp":1760643774051,"user_tz":240,"elapsed":2350,"user":{"displayName":"Logan Wong","userId":"05211338985266107031"}},"outputId":"bf71a472-88ef-439d-c8b2-740ceffa2928"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["\n","%cd /content/drive/MyDrive/MECE689_Bowling/MECE689_RL_Bowling_Atari\n","!ls -la"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qUMAIi3LHVDE","executionInfo":{"status":"ok","timestamp":1760643774168,"user_tz":240,"elapsed":115,"user":{"displayName":"Logan Wong","userId":"05211338985266107031"}},"outputId":"fb507560-3060-4c9e-fad8-6b0fb531d451"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/MECE689_Bowling/MECE689_RL_Bowling_Atari\n","total 30\n","drwx------ 2 root root 4096 Sep 25 15:17 checkpoints\n","drwx------ 2 root root 4096 Sep 25 15:17 code\n","drwx------ 2 root root 4096 Sep 25 15:04 .git\n","-rw------- 1 root root 6380 Oct 16 04:07 github_terminal.ipynb\n","-rw------- 1 root root   33 Sep 26 19:25 .gitignore\n","drwx------ 2 root root 4096 Sep 25 15:17 models\n","-rw------- 1 root root 2348 Sep 29 02:21 README.md\n","drwx------ 2 root root 4096 Sep 25 15:17 results\n"]}]},{"cell_type":"code","source":["!pip install gymnasium[atari,accept-rom-license] ale-py sb3_contrib stable-baselines3"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"R4AIf7YmHVFN","executionInfo":{"status":"ok","timestamp":1760643782790,"user_tz":240,"elapsed":7742,"user":{"displayName":"Logan Wong","userId":"05211338985266107031"}},"outputId":"bdce84fd-29f0-4f2e-a293-a2a9122196cd"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: ale-py in /usr/local/lib/python3.12/dist-packages (0.11.2)\n","Requirement already satisfied: sb3_contrib in /usr/local/lib/python3.12/dist-packages (2.7.0)\n","Requirement already satisfied: stable-baselines3 in /usr/local/lib/python3.12/dist-packages (2.7.0)\n","Requirement already satisfied: gymnasium[accept-rom-license,atari] in /usr/local/lib/python3.12/dist-packages (1.2.1)\n","\u001b[33mWARNING: gymnasium 1.2.1 does not provide the extra 'accept-rom-license'\u001b[0m\u001b[33m\n","\u001b[0mRequirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium[accept-rom-license,atari]) (2.0.2)\n","Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium[accept-rom-license,atari]) (3.1.1)\n","Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium[accept-rom-license,atari]) (4.15.0)\n","Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from gymnasium[accept-rom-license,atari]) (0.0.4)\n","Requirement already satisfied: torch<3.0,>=2.3 in /usr/local/lib/python3.12/dist-packages (from stable-baselines3) (2.8.0+cu126)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from stable-baselines3) (2.2.2)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from stable-baselines3) (3.10.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (3.20.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (75.2.0)\n","Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (1.13.3)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (3.5)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (2025.3.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (12.6.80)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (9.10.2.21)\n","Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (12.6.4.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (11.3.0.4)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (10.3.7.77)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (11.7.1.2)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (12.5.4.2)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (0.7.1)\n","Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (2.27.3)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (12.6.77)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (12.6.85)\n","Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (1.11.1.6)\n","Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (3.4.0)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->stable-baselines3) (1.3.3)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->stable-baselines3) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->stable-baselines3) (4.60.1)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->stable-baselines3) (1.4.9)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->stable-baselines3) (25.0)\n","Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->stable-baselines3) (11.3.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->stable-baselines3) (3.2.5)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib->stable-baselines3) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->stable-baselines3) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->stable-baselines3) (2025.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib->stable-baselines3) (1.17.0)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch<3.0,>=2.3->stable-baselines3) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch<3.0,>=2.3->stable-baselines3) (3.0.3)\n"]}]},{"cell_type":"code","source":["import os\n","import torch\n","import gymnasium as gym\n","import stable_baselines3\n","import ale_py\n","import numpy as np\n","\n","# RL Algorithm\n","from stable_baselines3 import A2C\n","\n","\n","# Visualization\n","from PIL import Image\n","import io\n","import base64\n","from IPython.display import display, HTML\n","\n","\n","\n","# For debugging\n","from stable_baselines3.common.monitor import Monitor\n","from stable_baselines3.common.callbacks import CheckpointCallback\n","from stable_baselines3.common.callbacks import BaseCallback\n","import time\n","\n","# Action masking\n","from gymnasium import ActionWrapper\n","from stable_baselines3.common.atari_wrappers import AtariWrapper\n","\n","# Vector environment\n","from stable_baselines3.common.env_util import make_atari_env\n","from stable_baselines3.common.vec_env import VecFrameStack, VecEnvWrapper, DummyVecEnv\n","from stable_baselines3.common.env_util import make_vec_env\n","\n","import gc\n","\n","print(\"All imports working\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sAp4aiOeHVHf","executionInfo":{"status":"ok","timestamp":1760643793647,"user_tz":240,"elapsed":10846,"user":{"displayName":"Logan Wong","userId":"05211338985266107031"}},"outputId":"7889c534-431b-42f4-850b-caa4e1dba760"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["All imports working\n"]},{"output_type":"stream","name":"stderr","text":["Gym has been unmaintained since 2022 and does not support NumPy 2.0 amongst other critical functionality.\n","Please upgrade to Gymnasium, the maintained drop-in replacement of Gym, or contact the authors of your software and request that they upgrade.\n","See the migration guide at https://gymnasium.farama.org/introduction/migration_guide/ for additional information.\n","/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n","  return datetime.utcnow().replace(tzinfo=utc)\n"]}]},{"cell_type":"code","source":["print(\"GPU available:\", torch.cuda.is_available())\n","if torch.cuda.is_available():\n","    print(\"GPU:\", torch.cuda.get_device_name(0))\n","else:\n","  print(\"CPU in use\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2g5Tbs0sHVJ5","executionInfo":{"status":"ok","timestamp":1760643793652,"user_tz":240,"elapsed":8,"user":{"displayName":"Logan Wong","userId":"05211338985266107031"}},"outputId":"0bf30b4e-f52b-456e-f291-3f2b17d3d9e0"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["GPU available: True\n","GPU: Tesla T4\n"]}]},{"cell_type":"code","source":["def convert(seconds):\n","    seconds = seconds % (24 * 3600)\n","    hour = seconds // 3600\n","    seconds %= 3600\n","    minutes = seconds // 60\n","    seconds %= 60\n","\n","    return \"%d:%02d:%02d\" % (hour, minutes, seconds)"],"metadata":{"id":"FMDMrbBzIYxS","executionInfo":{"status":"ok","timestamp":1760643793653,"user_tz":240,"elapsed":1,"user":{"displayName":"Logan Wong","userId":"05211338985266107031"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["class ActionReducer(ActionWrapper):\n","  def __init__(self, env):\n","    super().__init__(env)\n","\n","    # NOOP, FIRE, UP, and DOWN only. No UPFIRE. No DOWNFIRE.\n","    self.allowed_actions = [0,1,2,3]\n","\n","    self.action_space = gym.spaces.Discrete(len(self.allowed_actions))\n","\n","  def action(self, action):\n","    return self.allowed_actions[action]"],"metadata":{"id":"SIj4H3hsIYz9","executionInfo":{"status":"ok","timestamp":1760643793655,"user_tz":240,"elapsed":1,"user":{"displayName":"Logan Wong","userId":"05211338985266107031"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["def make_env():\n","  env = gym.make(\"ALE/Bowling-v5\")\n","  env = ActionReducer(env)\n","  env = Monitor(env)\n","  # disable reward clipping\n","  env = AtariWrapper(env, clip_reward=False)\n","  return env"],"metadata":{"id":"zDzUb5vKIY2l","executionInfo":{"status":"ok","timestamp":1760643793656,"user_tz":240,"elapsed":0,"user":{"displayName":"Logan Wong","userId":"05211338985266107031"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["seed = 316\n","torch.manual_seed(seed)\n","\n","env = DummyVecEnv([make_env])\n","env = VecFrameStack(env, n_stack=4)\n","\n","# num_of_parallel_envs = 4\n","# env = DummyVecEnv([make_env for _ in range(num_of_parallel_envs)])  # Parallel vector environments\n","# env = VecFrameStack(env, n_stack=4)\n","print(\"Env created\")\n","\n","# # GOAL: Compare A2C to Baseline DQN.\n","# # But... these 2 algorithms are pretty different\n","# # They only share 4 parameters: learning rate, gamma, verbose, and device\n","# # So really, only 2 parameters that matter...\n","# # Learning rate = 0.0001\n","# # gamma = 0.99\n","\n","# model = A2C(\n","#     \"CnnPolicy\",\n","#     env,\n","#     learning_rate=0.0001,\n","#     gamma=0.99,\n","#     verbose=1,\n","#     device=\"cuda\",\n","\n","#     n_steps=5,              # Steps per update; like DQN's train_freq=4\n","#     ent_coef=0.01,          # exploration (like epsilon-greedy)\n","#     gae_lambda=0.95,        # Standard value; provides slight bias for lower variance\n","#     vf_coef=0.5,            # Standard value; balance btwn actor & critic loss\n","#     max_grad_norm=0.5,      # Prevents exploding gradients\n","#     use_rms_prop=True       # RMSprop works well for Atari\n","# )\n","\n","# print(\"Actor 2 Critic model created\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"W7YMdFwJIY5Q","executionInfo":{"status":"ok","timestamp":1760643793919,"user_tz":240,"elapsed":262,"user":{"displayName":"Logan Wong","userId":"05211338985266107031"}},"outputId":"c7150bea-db04-4311-ebdc-a62f445d05bc"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Env created\n"]}]},{"cell_type":"code","source":["class SimpleCheckpointCallback(BaseCallback):\n","    def __init__(self, save_freq, save_path, name_prefix):\n","        super().__init__()\n","        self.save_freq = save_freq\n","        self.save_path = save_path\n","        self.name_prefix = name_prefix\n","\n","    def _on_step(self):\n","        if self.n_calls % self.save_freq == 0:\n","            path = os.path.join(self.save_path, f\"{self.name_prefix}_{self.n_calls}\")\n","            self.model.save(path)\n","            print(f\"Checkpoint saved at step {self.n_calls}\")\n","        return True"],"metadata":{"id":"biwe2xtRIY8Z","executionInfo":{"status":"ok","timestamp":1760643793924,"user_tz":240,"elapsed":4,"user":{"displayName":"Logan Wong","userId":"05211338985266107031"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"Gzw0rDr4e3MF","executionInfo":{"status":"ok","timestamp":1760643793925,"user_tz":240,"elapsed":4,"user":{"displayName":"Logan Wong","userId":"05211338985266107031"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"8flGOcJDe3Ou","executionInfo":{"status":"ok","timestamp":1760643793925,"user_tz":240,"elapsed":3,"user":{"displayName":"Logan Wong","userId":"05211338985266107031"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["def train_a2c_model(config_name, total_timesteps, learning_rate, gamma, n_steps, ent_coef):\n","    print(f\"Testing {config_name}...\")\n","\n","    model = A2C(\n","        \"CnnPolicy\",\n","        env,\n","\n","        learning_rate=learning_rate,\n","        gamma=gamma,\n","\n","        n_steps=n_steps,\n","        ent_coef=ent_coef,\n","\n","        gae_lambda=0.99,\n","        vf_coef=0.5,\n","        max_grad_norm=0.5,\n","        use_rms_prop=True,\n","        verbose=0,\n","        device=\"cuda\"\n","    )\n","\n","    # Train for a short time to compare\n","    model.learn(total_timesteps=total_timesteps, progress_bar=True)\n","\n","    return model"],"metadata":{"id":"7hyu0Qj9cIHV","executionInfo":{"status":"ok","timestamp":1760643793926,"user_tz":240,"elapsed":3,"user":{"displayName":"Logan Wong","userId":"05211338985266107031"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["def evaluate_model(model, env, n_eval_episodes):\n","    \"\"\"Evaluate model and return mean reward\"\"\"\n","    rewards = []\n","    action_dict = {\n","      0: \"NOOP\",\n","      1: \"FIRE\",\n","      2: \"UP\",\n","      3: \"DOWN\"\n","    }\n","    for episode in range(n_eval_episodes):\n","        obs = env.reset()\n","        episode_reward = 0\n","        done = False\n","        total_reward = 0\n","        steps = 0\n","\n","        while not done:\n","            action, _ = model.predict(obs, deterministic=True)\n","            # Actions: NOOP(0), FIRE(1), UP(2), and DOWN(3)\n","            obs, reward, done, _ = env.step(action)\n","            # if reward[0] > 0:\n","            #   print(f\"Reward earned for doing {action_dict[action[0]]}: {reward[0]}\")\n","            episode_reward += reward[0]\n","            steps += 1\n","\n","        rewards.append(episode_reward)\n","        print(f\"Episode {episode+1}: Reward = {episode_reward:6.1f}, Steps = {steps}\")\n","\n","    return np.mean(rewards)"],"metadata":{"id":"4XEvBOj6dM7g","executionInfo":{"status":"ok","timestamp":1760643793927,"user_tz":240,"elapsed":0,"user":{"displayName":"Logan Wong","userId":"05211338985266107031"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["# Test different configurations\n","\n","#     learning_rate=0.0001,\n","#     gamma=0.99,\n","#     n_steps=5,              # Steps per update; like DQN's train_freq=4\n","#     ent_coef=0.01,          # exploration (like epsilon-greedy)\n","#     gae_lambda=0.95,        # Standard value; provides slight bias for lower variance\n","#     vf_coef=0.5,            # Standard value; balance btwn actor & critic loss\n","# Vf_coef Balances actor (decision maker) and critic (value estimator). SO 0.5 balances both.\n","\n","\n","# gamma=0.999,          # Longer horizon for sparse rewards\n","# n_steps=10,           # More steps per update\n","# ent_coef=0.1,         # Much more exploration\n","\n","# learning rate: 0.0001 = slow/steady vs. 0.001 = fast/jumpy\n","# n steps: Higher is better for sparse rewards. 5, 10, 15, 20, 25, 30, 35, 40, 45, 50\n","# ent coef: LOW (0.01) = less exploration, HIGH (0.1) = more exploration\n","\n","# gae lambda:  Low (0.95) = Short-term matters more, High (0.99) = Long-term matters more (Better for Sparse)... I'll just keep it at 0.99\n","# vf coef: i'll just keep it at 0.5\n","# learning_rate, gamma, n_steps, ent_coef, gae_lambda\n","# configs = {\n","#     \"conservative\":   (0.0001, 5, 0.01, 0.95, 0.5),\n","#     \"more_explore\":   (0.0001, 5, 0.1, 0.95, 0.5),\n","#     \"longer_horizon\": (0.0001, 10, 0.01, 0.99, 0.5),\n","#     \"higher_lr\":      (0.001, 5, 0.01, 0.95, 0.5),\n","# }\n","\n","# Learning rate: 0.0001 to 0.001\n","# gamma: 0.99 to 0.999\n","# n_steps: 5, 10, 15, 20, 25, 30, 35, 40, 45, 50\n","# ent_coef: 0.01 to 0.1\n","# game_lambda:0.95 to 0.99\n","\n","\n","configs = {}\n","config_id = 1\n","\n","# Define the ranges and step sizes\n","learning_rates = [0.0001, 0.0005, 0.001]\n","gammas = [0.99, 0.995, 0.999]\n","n_steps_list = [5, 10, 15, 30, 40, 50]\n","ent_coefs = [0.01, 0.05, 0.1]\n","\n","# Generate all combinations\n","for lr in learning_rates:\n","    for gamma in gammas:\n","        for n_steps in n_steps_list:\n","            for ent_coef in ent_coefs:\n","                config_name = f\"config_{config_id}\"\n","                configs[config_name] = (lr, gamma, n_steps, ent_coef)\n","                config_id += 1\n","\n","print(f\"Generated {len(configs)} configurations\")\n","# for config in configs:\n","#   print(configs[config])\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"_bxd1r7WcBZQ","executionInfo":{"status":"ok","timestamp":1760643793983,"user_tz":240,"elapsed":4,"user":{"displayName":"Logan Wong","userId":"05211338985266107031"}},"outputId":"93e080cd-9eba-4dda-984e-ba3ea73964eb"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["Generated 162 configurations\n"]}]},{"cell_type":"code","source":["# total_timesteps = 10000000    # 10M\n","# total_timesteps = 5000000    # 5M\n","# total_timesteps =  2000000    # 2M\n","# total_timesteps =  1000000    # 1M\n","# total_timesteps =   100000    # 100K\n","total_timesteps =   50000    # 50K\n","# total_timesteps =    10000    # 10K\n","# total_timesteps =     5000    # 5K\n","\n","# n_eval_episodes = 10\n","n_eval_episodes = 30\n","# n_eval_episodes = 100\n","\n","best_reward = -float('inf')\n","best_config = None\n","\n","for name, params in configs.items():\n","    model = train_a2c_model(name, total_timesteps, *params)\n","    print(\"Model done training\")\n","\n","    reward = evaluate_model(model, env, n_eval_episodes)\n","    print(f\"{name}: {reward:.1f} mean reward\")\n","\n","    if reward > best_reward:\n","        best_reward = reward\n","        best_config = name\n","\n","print(f\"Best config: {best_config} with reward {best_reward}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["8a15214340b84d34bd86f444bcc43083","1e5bba4628d24218be669ed03436da39"]},"id":"tqOoqy_6CUnV","executionInfo":{"status":"error","timestamp":1760644266892,"user_tz":240,"elapsed":472907,"user":{"displayName":"Logan Wong","userId":"05211338985266107031"}},"outputId":"1dfcb6d7-96be-431a-d9c6-f06af59e9a8c"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["Testing config_1...\n"]},{"output_type":"display_data","data":{"text/plain":["/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: \n","datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects \n","to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n","  return datetime.utcnow().replace(tzinfo=utc)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: \n","datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects \n","to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n","  return datetime.utcnow().replace(tzinfo=utc)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Output()"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8a15214340b84d34bd86f444bcc43083"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["/usr/local/lib/python3.12/dist-packages/ipywidgets/widgets/widget_output.py:111: DeprecationWarning: \n","Kernel._parent_header is deprecated in ipykernel 6. Use .get_parent()\n","  if ip and hasattr(ip, 'kernel') and hasattr(ip.kernel, '_parent_header'):\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.12/dist-packages/ipywidgets/widgets/widget_output.py:111: DeprecationWarning: \n","Kernel._parent_header is deprecated in ipykernel 6. Use .get_parent()\n","  if ip and hasattr(ip, 'kernel') and hasattr(ip.kernel, '_parent_header'):\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":[],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Model done training\n","Episode 1: Reward =    0.0, Steps = 124\n","Episode 2: Reward =    3.0, Steps = 498\n","Episode 3: Reward =    3.0, Steps = 498\n","Episode 4: Reward =    3.0, Steps = 498\n","Episode 5: Reward =    3.0, Steps = 498\n","Episode 6: Reward =    3.0, Steps = 498\n","Episode 7: Reward =    3.0, Steps = 498\n","Episode 8: Reward =    3.0, Steps = 498\n","Episode 9: Reward =    3.0, Steps = 498\n","Episode 10: Reward =    3.0, Steps = 498\n","Episode 11: Reward =    3.0, Steps = 498\n","Episode 12: Reward =    3.0, Steps = 498\n","Episode 13: Reward =    3.0, Steps = 498\n","Episode 14: Reward =    3.0, Steps = 498\n","Episode 15: Reward =    3.0, Steps = 498\n","Episode 16: Reward =    3.0, Steps = 498\n","Episode 17: Reward =    3.0, Steps = 498\n","Episode 18: Reward =    3.0, Steps = 498\n","Episode 19: Reward =    3.0, Steps = 498\n","Episode 20: Reward =    3.0, Steps = 498\n","Episode 21: Reward =    3.0, Steps = 498\n","Episode 22: Reward =    3.0, Steps = 498\n","Episode 23: Reward =    3.0, Steps = 498\n","Episode 24: Reward =    3.0, Steps = 498\n","Episode 25: Reward =    3.0, Steps = 498\n","Episode 26: Reward =    3.0, Steps = 498\n","Episode 27: Reward =    3.0, Steps = 498\n","Episode 28: Reward =    3.0, Steps = 498\n","Episode 29: Reward =    3.0, Steps = 498\n","Episode 30: Reward =    3.0, Steps = 498\n","Episode 31: Reward =    3.0, Steps = 498\n","Episode 32: Reward =    3.0, Steps = 498\n","Episode 33: Reward =    3.0, Steps = 498\n","Episode 34: Reward =    3.0, Steps = 498\n","Episode 35: Reward =    3.0, Steps = 498\n","Episode 36: Reward =    3.0, Steps = 498\n","Episode 37: Reward =    3.0, Steps = 498\n","Episode 38: Reward =    6.0, Steps = 498\n","Episode 39: Reward =    3.0, Steps = 498\n","Episode 40: Reward =    3.0, Steps = 498\n","Episode 41: Reward =    3.0, Steps = 498\n","Episode 42: Reward =    3.0, Steps = 498\n","Episode 43: Reward =    3.0, Steps = 498\n","Episode 44: Reward =    3.0, Steps = 498\n","Episode 45: Reward =    3.0, Steps = 498\n","Episode 46: Reward =    3.0, Steps = 498\n","Episode 47: Reward =    3.0, Steps = 498\n","Episode 48: Reward =    3.0, Steps = 498\n","Episode 49: Reward =    3.0, Steps = 498\n","Episode 50: Reward =    3.0, Steps = 498\n","Episode 51: Reward =    3.0, Steps = 498\n","Episode 52: Reward =    3.0, Steps = 498\n","Episode 53: Reward =    3.0, Steps = 498\n","Episode 54: Reward =    3.0, Steps = 498\n","Episode 55: Reward =    3.0, Steps = 498\n","Episode 56: Reward =    3.0, Steps = 498\n","Episode 57: Reward =    3.0, Steps = 498\n","Episode 58: Reward =    3.0, Steps = 498\n","Episode 59: Reward =    3.0, Steps = 498\n","Episode 60: Reward =    3.0, Steps = 498\n","Episode 61: Reward =    3.0, Steps = 498\n","Episode 62: Reward =    3.0, Steps = 498\n","Episode 63: Reward =    3.0, Steps = 498\n","Episode 64: Reward =    3.0, Steps = 498\n","Episode 65: Reward =    3.0, Steps = 498\n","Episode 66: Reward =    3.0, Steps = 498\n","Episode 67: Reward =    3.0, Steps = 498\n","Episode 68: Reward =    3.0, Steps = 498\n","Episode 69: Reward =    3.0, Steps = 498\n","Episode 70: Reward =    3.0, Steps = 498\n","Episode 71: Reward =    3.0, Steps = 498\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-2752003475.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Model done training\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_eval_episodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{name}: {reward:.1f} mean reward\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipython-input-363131401.py\u001b[0m in \u001b[0;36mevaluate_model\u001b[0;34m(model, env, n_eval_episodes)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m             \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeterministic\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m             \u001b[0;31m# Actions: NOOP(0), FIRE(1), UP(2), and DOWN(3)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/stable_baselines3/common/base_class.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, observation, state, episode_start, deterministic)\u001b[0m\n\u001b[1;32m    555\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0mused\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrecurrent\u001b[0m \u001b[0mpolicies\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    556\u001b[0m         \"\"\"\n\u001b[0;32m--> 557\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepisode_start\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeterministic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    558\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mset_random_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/stable_baselines3/common/policies.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, observation, state, episode_start, deterministic)\u001b[0m\n\u001b[1;32m    366\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    367\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 368\u001b[0;31m             \u001b[0mactions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeterministic\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdeterministic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    369\u001b[0m         \u001b[0;31m# Convert to numpy, and reshape to the original action shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m         \u001b[0mactions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc, assignment]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/stable_baselines3/common/policies.py\u001b[0m in \u001b[0;36m_predict\u001b[0;34m(self, observation, deterministic)\u001b[0m\n\u001b[1;32m    715\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0;32mreturn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTaken\u001b[0m \u001b[0maction\u001b[0m \u001b[0maccording\u001b[0m \u001b[0mto\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    716\u001b[0m         \"\"\"\n\u001b[0;32m--> 717\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_distribution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_actions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdeterministic\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdeterministic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    718\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    719\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mevaluate_actions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mPyTorchObs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/stable_baselines3/common/policies.py\u001b[0m in \u001b[0;36mget_distribution\u001b[0;34m(self, obs)\u001b[0m\n\u001b[1;32m    748\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0;32mreturn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0maction\u001b[0m \u001b[0mdistribution\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    749\u001b[0m         \"\"\"\n\u001b[0;32m--> 750\u001b[0;31m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpi_features_extractor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    751\u001b[0m         \u001b[0mlatent_pi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmlp_extractor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_actor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    752\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_action_dist_from_latent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlatent_pi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/stable_baselines3/common/policies.py\u001b[0m in \u001b[0;36mextract_features\u001b[0;34m(self, obs, features_extractor)\u001b[0m\n\u001b[1;32m    129\u001b[0m         \"\"\"\n\u001b[1;32m    130\u001b[0m         \u001b[0mpreprocessed_obs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess_obs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation_space\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalize_images\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_images\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfeatures_extractor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreprocessed_obs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_constructor_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/stable_baselines3/common/torch_layers.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, observations)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobservations\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1776\u001b[0m     \u001b[0;31m# fmt: off\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1777\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1778\u001b[0m         \u001b[0mforward_call\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1779\u001b[0m         \u001b[0;31m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","source":[],"metadata":{"id":"aXR7dyk3cCy4","executionInfo":{"status":"aborted","timestamp":1760644266895,"user_tz":240,"elapsed":0,"user":{"displayName":"Logan Wong","userId":"05211338985266107031"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# # Save couple of steps\n","# checkpoint_callback = CheckpointCallback(\n","#     save_freq=100000,\n","#     save_path=\"/content/drive/MyDrive/MECE689_Bowling/MECE689_RL_Bowling_Atari/checkpoints\",\n","#     name_prefix=f\"a2c_{total_timesteps}\"\n","# )\n","\n","# # Time how long it takes\n","# print(\"Training started\")\n","# start_time = time.time()\n","\n","# # log_interval: Print train metrics every 10 episodes\n","# model.learn(\n","#     total_timesteps=total_timesteps,\n","#     callback=checkpoint_callback,\n","#     progress_bar=True,\n","#     log_interval=10\n","# )\n","# end_time = time.time()\n","# print(\"Training done\")\n","\n","# env.close()\n","\n","# # Calculate run time\n","# training_duration = end_time - start_time\n","# time_in_minutes_and_seconds = convert(training_duration)\n","# print(f\"Time taken: {time_in_minutes_and_seconds}\")\n","# # print(f\"Speed: {total_timesteps/training_duration:.2f} steps/second\")\n","\n","# # Save model to Google Drive\n","# trained_model_save_path = f\"/content/drive/MyDrive/MECE689_Bowling/MECE689_RL_Bowling_Atari/models/a2c_HP_Tuning_{total_timesteps}\"\n","# model.save(trained_model_save_path)\n","# print(\"Model saved to Google Drive\")"],"metadata":{"collapsed":true,"id":"BCAgM-DNHVMg","executionInfo":{"status":"aborted","timestamp":1760644266900,"user_tz":240,"elapsed":4,"user":{"displayName":"Logan Wong","userId":"05211338985266107031"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# torch.cuda.empty_cache()\n","# del model\n","# del env"],"metadata":{"id":"L-fdxFlFcAr_","executionInfo":{"status":"aborted","timestamp":1760644266901,"user_tz":240,"elapsed":495314,"user":{"displayName":"Logan Wong","userId":"05211338985266107031"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"TbtwTAOfIIM0","executionInfo":{"status":"aborted","timestamp":1760644266902,"user_tz":240,"elapsed":495313,"user":{"displayName":"Logan Wong","userId":"05211338985266107031"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"rqldZcJjIIPZ","executionInfo":{"status":"aborted","timestamp":1760644266902,"user_tz":240,"elapsed":495311,"user":{"displayName":"Logan Wong","userId":"05211338985266107031"}}},"execution_count":null,"outputs":[]}]}